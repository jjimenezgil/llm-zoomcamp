{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215026ce-c5bf-413a-8646-21b5d5f8fb0e",
   "metadata": {},
   "source": [
    "# Homework: Evaluation and Monitoring\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e385e263-69a2-4357-ba52-62d7c8622f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import requests \n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf980c8-184a-4ec1-9e12-ac16749c4a16",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model\n",
    "\n",
    "In first place, let's get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b7c74e-e3c6-4a97-9ec6-cc6c582fd786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You can sign up for the course by visiting the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Where can I sign up for the course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You can sign up using the link provided in the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Can you provide a link to sign up?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, there is an FAQ for the Machine Learning ...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Is there an FAQ for this Machine Learning course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The context does not provide any specific info...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Does this course have a GitHub repository for ...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To structure your questions and answers for th...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>How can I structure my questions and answers f...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          answer_llm  \\\n",
       "0  You can sign up for the course by visiting the...   \n",
       "1  You can sign up using the link provided in the...   \n",
       "2  Yes, there is an FAQ for the Machine Learning ...   \n",
       "3  The context does not provide any specific info...   \n",
       "4  To structure your questions and answers for th...   \n",
       "\n",
       "                                         answer_orig  document  \\\n",
       "0  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "1  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "2  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "3  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "4  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "\n",
       "                                            question  \\\n",
       "0                Where can I sign up for the course?   \n",
       "1                 Can you provide a link to sign up?   \n",
       "2  Is there an FAQ for this Machine Learning course?   \n",
       "3  Does this course have a GitHub repository for ...   \n",
       "4  How can I structure my questions and answers f...   \n",
       "\n",
       "                      course  \n",
       "0  machine-learning-zoomcamp  \n",
       "1  machine-learning-zoomcamp  \n",
       "2  machine-learning-zoomcamp  \n",
       "3  machine-learning-zoomcamp  \n",
       "4  machine-learning-zoomcamp  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f9302-361a-422b-84c3-1cd85a60d236",
   "metadata": {},
   "source": [
    "We will use only the first 300 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753d3b01-bfda-49ab-a3b3-375d272a7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4f2c3-86bf-4d07-a46c-19e6061aab30",
   "metadata": {},
   "source": [
    "Now that we have the data, we can get the model that will generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f533ecdd-a791-4cc8-abef-2cce3fd7881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5175985-963d-43f5-baf5-af4397dde821",
   "metadata": {},
   "source": [
    "We will use this model to compute the embedding of the LLM answer in the first row of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3344b1-d28e-4ced-ad1c-9606e0b49f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.42244655\n"
     ]
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "answer_llm_emb = embedding_model.encode(answer_llm)\n",
    "print(answer_llm_emb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b33fe8-e54b-467f-afcd-71e4ee29bd8d",
   "metadata": {},
   "source": [
    "**Answer**: -0.42\n",
    "\n",
    "\n",
    "## Q2. Computing the dot product\n",
    "\n",
    "We will compute the similarity between each pair (LLM answer and original answer) using the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92077b20-54e3-40c2-a711-3452f23c4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9e13c1006e49f587ffe442f1bfea7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert dataframe to dict\n",
    "df_dict = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Compute dot product and save all the results in a list\n",
    "evaluations = []\n",
    "\n",
    "for doc in tqdm(df_dict):\n",
    "    # Compute embeddings\n",
    "    doc[\"answer_llm_emb\"] = embedding_model.encode(doc[\"answer_llm\"])\n",
    "    doc[\"answer_orig_emb\"] = embedding_model.encode(doc[\"answer_orig\"])\n",
    "    # Compute dot product\n",
    "    evaluations.append(np.dot(doc[\"answer_llm_emb\"], doc[\"answer_orig_emb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f78d23-5925-4334-b272-d7696557bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.67430877685547\n"
     ]
    }
   ],
   "source": [
    "# Convert list to array and calculate 75%\n",
    "eval_array = np.asarray(evaluations)\n",
    "percentile = np.percentile(eval_array, 75)\n",
    "print(percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e84cd9-9215-4ef1-8bdd-14a2265ce7a9",
   "metadata": {},
   "source": [
    "**Answer**: 31.67\n",
    "\n",
    "\n",
    "## Q3. Computing the cosine\n",
    "\n",
    "We can see that the results are not between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11d7d81-ed5d-4971-993e-6f2c4440d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.515987, 13.418402, 25.313255, 12.147415, 18.747736, 33.970406, 30.251705, 29.521576, 35.272198, 27.751772]\n"
     ]
    }
   ],
   "source": [
    "print(evaluations[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fec812-ce26-4c68-b804-946fed398dd2",
   "metadata": {},
   "source": [
    "The model used to generate the embeddings does not generate normalized vectors, so let's normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0b1b4f-1769-40ca-a96d-cc8ea09e440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebe125b-fd79-4155-9d01-3a308a9e4222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5943514a6e4b8aa82312227bc546cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for doc in tqdm(df_dict):\n",
    "    # Compute normalization of the embeddings\n",
    "    doc[\"answer_llm_emb_norm\"] = normalize_vec(doc[\"answer_llm_emb\"])\n",
    "    doc[\"answer_orig_emb_norm\"] = normalize_vec(doc[\"answer_orig_emb\"])\n",
    "    # Compute dot product\n",
    "    evaluations.append(np.dot(doc[\"answer_llm_emb_norm\"], doc[\"answer_orig_emb_norm\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815d97e0-c37b-448a-b1a1-9099eb1ae58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8362348973751068\n"
     ]
    }
   ],
   "source": [
    "# Convert list to array and calculate 75%\n",
    "eval_array = np.asarray(evaluations)\n",
    "percentile = np.percentile(eval_array, 75)\n",
    "print(percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18388b-2024-471c-859e-a838b38de19b",
   "metadata": {},
   "source": [
    "**Answer**: 0.83\n",
    "\n",
    "\n",
    "## Q4. Rouge\n",
    "\n",
    "Let's now use a different similarity metric, Rouge score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5fa3af5-5ad7-4a3a-9fab-2262ef1deae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454544954545456\n"
     ]
    }
   ],
   "source": [
    "rouge_scorer = Rouge()\n",
    "scores = rouge_scorer.get_scores(df_dict[10]['answer_llm'], df_dict[10]['answer_orig'])[0]\n",
    "print(scores['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b64ff3-c529-4e4d-8dd1-206e466e40dc",
   "metadata": {},
   "source": [
    "**Answer**: 0.45\n",
    "\n",
    "\n",
    "## Q5. Average rouge score\n",
    "\n",
    "We need to compute the average between rouge-1, rouge-2 and rouge-l for the same record in Q4, but it's not specified if only for the f-score or for all the metrics (f-score, precission and recall). So let's compute the both cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70cfed73-7414-42a1-bcef-6d676100eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35490034990035496\n",
      "0.3549003532336883\n"
     ]
    }
   ],
   "source": [
    "def avg_rouge_f_score(scores):\n",
    "    return (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f'])/3\n",
    "\n",
    "def avg_rouge_total_score(scores):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    \n",
    "    for k, v in scores.items():\n",
    "        for x, y in v.items():\n",
    "            count = count + 1\n",
    "            total = total + y\n",
    "\n",
    "    return total/count\n",
    "\n",
    "\n",
    "print(avg_rouge_f_score(scores))\n",
    "print(avg_rouge_total_score(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19909703-a994-46ad-8290-bae9cfc8fe33",
   "metadata": {},
   "source": [
    "In this case, both averages are more or less the same.\n",
    "\n",
    "**Answer**: 0.35\n",
    "\n",
    "\n",
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "In this case it is specified that we need to calculate the Rouge-2 f-score average for all the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6beda9df-3acc-4d2f-9a61-57edf56b255b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7e204d63314aab99f9cf1bf385806a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_2_f_scores = []\n",
    "\n",
    "for doc in tqdm(df_dict):\n",
    "    scores = rouge_scorer.get_scores(doc['answer_llm'], doc['answer_orig'])[0]\n",
    "    rouge_2_f_scores.append(scores['rouge-2']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e941951-5a2e-4828-92df-2d36b6898224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20696501983423318\n"
     ]
    }
   ],
   "source": [
    "avg = sum(rouge_2_f_scores)/len(rouge_2_f_scores)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84369024-e796-439e-8644-9265bacf935b",
   "metadata": {},
   "source": [
    "**Answer**: 0.20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
